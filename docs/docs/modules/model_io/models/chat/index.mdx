---
sidebar_position: 1
---

import CodeBlock from "@theme/CodeBlock";

# Chat models

:::info
Head to [Integrations](/docs/integrations/chat) for documentation on built-in integrations with chat model providers.
:::

Chat models are a variation on language models.
While chat models use language models under the hood, the interface they expose is a bit different.
Rather than expose a "text in, text out" API, they expose an interface where "chat messages" are the inputs and outputs.

Chat model APIs are fairly new, so we are still figuring out the correct abstractions.

The following sections of documentation are provided:

- **How-to guides**: Walkthroughs of core functionality, like streaming, creating chat prompts, etc.

- **Integrations**: How to use different chat model providers (OpenAI, Anthropic, etc).

## Get started

### Setup

Accessing the API requires an API key, which you can get by creating an account and heading [here](https://platform.openai.com/account/api-keys). Once we have a key we'll want to set it as an environment variable by running:

```bash
export OPENAI_API_KEY="..."
```

If you'd prefer not to set an environment variable you can pass the key in directly via the `openAIApiKey` parameter when initializing the ChatOpenAI class:

```typescript
import { ChatOpenAI } from "langchain/chat_models/openai";

const chat = new ChatOpenAI({
  openAIApiKey: "YOUR_KEY_HERE",
});
```

otherwise you can initialize it with an empty object:

```typescript
import { ChatOpenAI } from "langchain/chat_models/openai";

const chat = new ChatOpenAI({});
```

### Messages

The chat model interface is based around messages rather than raw text.
The types of messages currently supported in LangChain are `AIMessage`, `HumanMessage`, `SystemMessage`, `FunctionMessage`, and `ChatMessage` -- `ChatMessage` takes in an arbitrary role parameter. Most of the time, you'll just be dealing with `HumanMessage`, `AIMessage`, and `SystemMessage`

### `invoke`

#### Generic inputs -> generic outputs

You can generate LLM responses by calling `.invoke` and passing in whatever inputs you defined in the [`Runnable`](/docs/expression_language/).

import RunnableExample from "@examples/models/chat/runnable_chat_quick_start.ts";

<CodeBlock language="typescript">{RunnableExample}</CodeBlock>

### `call`

#### Messages in -> message out

You can get chat completions by passing one or more messages to the chat model. The response will be a message.

import Example from "@examples/models/chat/chat_quick_start.ts";

<CodeBlock language="typescript">{Example}</CodeBlock>

OpenAI's chat model also supports multiple messages as input. See [here](https://platform.openai.com/docs/guides/chat/chat-vs-completions) for more information. Here is an example of sending a system and user message to the chat model:

```typescript
const response2 = await chat.call([
  new SystemMessage(
    "You are a helpful assistant that translates English to French."
  ),
  new HumanMessage("Translate: I love programming."),
]);
console.log(response2);
// AIMessage { text: "J'aime programmer." }
```

### `generate`

#### Batch calls, richer outputs

You can go one step further and generate completions for multiple sets of messages using `generate`. This returns an `LLMResult` with an additional `message` parameter.

```typescript
const response3 = await chat.generate([
  [
    new SystemMessage(
      "You are a helpful assistant that translates English to French."
    ),
    new HumanMessage(
      "Translate this sentence from English to French. I love programming."
    ),
  ],
  [
    new SystemMessage(
      "You are a helpful assistant that translates English to French."
    ),
    new HumanMessage(
      "Translate this sentence from English to French. I love artificial intelligence."
    ),
  ],
]);
console.log(response3);
/*
  {
    generations: [
      [
        {
          text: "J'aime programmer.",
          message: AIMessage { text: "J'aime programmer." },
        }
      ],
      [
        {
          text: "J'aime l'intelligence artificielle.",
          message: AIMessage { text: "J'aime l'intelligence artificielle." }
        }
      ]
    ]
  }
*/
```

You can recover things like token usage from this LLMResult:

```typescript
console.log(response3.llmOutput);
/*
  {
    tokenUsage: { completionTokens: 20, promptTokens: 69, totalTokens: 89 }
  }
*/
```
